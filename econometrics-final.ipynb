{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the code used for our econometrics project. This notebook will be nonfunctioning because of the absurd time requirements of harvesting data (the code would take over 24 hours to run in total, and would not be over the correct time interval).\n",
    "\n",
    "Nevertheless, all the code used for collecting will be there, commented out, and all cleaning and model work will be included. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_execution_state": "idle",
    "_uuid": "051d70d956493feee0c6d64651c6a088724dca2a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Installing package into ‘/usr/local/lib/R/site-library’\n",
      "(as ‘lib’ is unspecified)\n",
      "\n",
      "also installing the dependencies ‘lexicon’, ‘syuzhet’, ‘textclean’\n",
      "\n",
      "\n",
      "\n",
      "sentimentr installed\n",
      "\n",
      "Installing package into ‘/usr/local/lib/R/site-library’\n",
      "(as ‘lib’ is unspecified)\n",
      "\n",
      "also installing the dependencies ‘dials’, ‘infer’, ‘parsnip’, ‘tidypredict’, ‘tidyposterior’, ‘tune’, ‘workflows’, ‘yardstick’\n",
      "\n",
      "\n",
      "\n",
      "tidymodels installed\n",
      "\n"
     ]
    },
    {
     "ename": "ERROR",
     "evalue": "Error in twitter_init_oauth1.0(self$endpoint, self$app, permission = self$params$permission, : Unauthorized (HTTP 401).\n",
     "output_type": "error",
     "traceback": [
      "Error in twitter_init_oauth1.0(self$endpoint, self$app, permission = self$params$permission, : Unauthorized (HTTP 401).\nTraceback:\n",
      "1. create_token(app = \"twittersentimentprimary\", consumer_key = twitter_api_key, \n .     consumer_secret = api_secret_key)",
      "2. create_token_(app, consumer_key, consumer_secret, access_token, \n .     access_secret, set_renv)",
      "3. twitter_Token1.0$new(app = app, endpoint = httr::oauth_endpoints(\"twitter\"), \n .     cache = FALSE)",
      "4. .subset2(public_bind_env, \"initialize\")(...)",
      "5. self$init_credentials()",
      "6. twitter_init_oauth1.0(self$endpoint, self$app, permission = self$params$permission, \n .     private_key = self$private_key)",
      "7. httr::stop_for_status(response)"
     ]
    }
   ],
   "source": [
    "rm(list=ls())\n",
    "library(pacman)\n",
    "p_load(tidyverse, rtweet,sentimentr,data.table,lubridate,janitor,tidymodels,parsnip,magrittr,stringr,estimatr,wordcloud)\n",
    "\n",
    "## store api keys\n",
    "twitter_api_key <- \"XX\" #api key obtained from twitter\n",
    "api_secret_key <- \"XX\"\n",
    "\n",
    "## authenticate via web browser\n",
    "token <- create_token(\n",
    "  app = \"twittersentimentprimary\", # the name of my app\n",
    "  consumer_key = twitter_api_key,\n",
    "  consumer_secret = api_secret_key)\n",
    "\n",
    "# test tweet search\n",
    "#rt <- search_tweets(\n",
    "#  \"#bernie\", n = 10, include_rts = FALSE\n",
    "#)\n",
    "\n",
    "# Test the state lookup. Here we created a google geolocation API account\n",
    "oregon <- lookup_coords(address=\"oregon\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Note that it takes some time to do these, be purposeful\n",
    "# This code separates the tweets into individual sentences\n",
    "sentences <- get_sentences(rt$text)\n",
    "# This getst the sentiment over those sentences\n",
    "sent <- sentiment(sentences)\n",
    "# This gets the average sentiment\n",
    "mean(sent$sentiment)\n",
    "# Look at a specific tweet, make sure they have come in properly\n",
    "rt$text[17950]\n",
    "# Plot for better understanding of when people are tweeting about bernie\n",
    "nvbernie %>%\n",
    "  ts_plot(\"3 hours\") +\n",
    "  ggplot2::theme_minimal() +\n",
    "  ggplot2::theme(plot.title = ggplot2::element_text(face = \"bold\")) +\n",
    "  ggplot2::labs(\n",
    "    x = NULL, y = NULL,\n",
    "    title = \"Frequency of #bernie Twitter statuses from past 9 days\",\n",
    "    subtitle = \"Twitter status (tweet) counts aggregated using three-hour intervals\"\n",
    "  )\n",
    "\n",
    "# The following code was for the preliminary analyses. Sys.time() tells the time of the system when the code is executed\n",
    "# since these datapoints can take hours to collect, it is nice to know how long it has been running\n",
    "Sys.time()\n",
    "nhbernie <- search_tweets(\n",
    "  \"#bernie OR #bernie2020 OR bernie OR sanders\", geocode = lookup_coords(address=\"new hampshire\"), n = 100000,\n",
    "  include_rts = FALSE, retryonratelimit = TRUE\n",
    ")\n",
    "Sys.time()\n",
    "# Checks the dimmensions of the data, see how many tweets we got\n",
    "dim(nhbernie)\n",
    "# Write to a file\n",
    "fwrite(nhbernie,file=\"C:/Users/Evanm/Documents/nhbernie.csv\")\n",
    "\n",
    "# The same is done for all other candidates below.\n",
    "# this was just a preliminary test, a loop is created later.\n",
    "\n",
    "Sys.time()\n",
    "nhbiden <- search_tweets(\n",
    "  \"#biden OR #biden2020 OR biden OR joe\", geocode = lookup_coords(address=\"new hampshire\"), n = 100000,\n",
    "  include_rts = FALSE, retryonratelimit = TRUE\n",
    ")\n",
    "Sys.time()\n",
    "dim(nhbiden)\n",
    "fwrite(nhbiden,file=\"C:/Users/Evanm/Documents/nhbiden.csv\")\n",
    "\n",
    "Sys.time()\n",
    "nhklobuchar <- search_tweets(\n",
    "  \"#klobuchar OR #klobuchar2020 OR klobuchar OR amy\", geocode = lookup_coords(address=\"new hampshire\"), n = 100000,\n",
    "  include_rts = FALSE, retryonratelimit = TRUE\n",
    ")\n",
    "Sys.time()\n",
    "dim(nhklobuchar)\n",
    "fwrite(nhklobuchar,file=\"C:/Users/Evanm/Documents/nhklobuchar.csv\")\n",
    "\n",
    "Sys.time()\n",
    "nhbuttigieg <- search_tweets(\n",
    "  \"#buttigieg OR #pete OR #buttigieg2020 OR #pete2020 OR buttigieg OR pete\", geocode = lookup_coords(address=\"new hampshire\"), n = 100000,\n",
    "  include_rts = FALSE, retryonratelimit = TRUE\n",
    ")\n",
    "Sys.time()\n",
    "dim(nhbuttigieg)\n",
    "fwrite(nhbuttigieg,file=\"C:/Users/Evanm/Documents/nhbuttigieg.csv\")\n",
    "\n",
    "Sys.time()\n",
    "nhwarren <- search_tweets(\n",
    "  \"#warren OR or #warren2020 OR warren OR elizabeth\", geocode = lookup_coords(address=\"new hampshire\"), n = 100000,\n",
    "  include_rts = FALSE, retryonratelimit = TRUE\n",
    ")\n",
    "Sys.time()\n",
    "dim(nhwarren)\n",
    "fwrite(nhwarren,file=\"C:/Users/Evanm/Documents/nhwarren.csv\")\n",
    "\n",
    "Sys.time()\n",
    "nhsteyer <- search_tweets(\n",
    "  \"#steyer OR #steyer2020 OR steyer OR tom\", geocode = lookup_coords(address=\"new hampshire\"), n = 100000,\n",
    "  include_rts = FALSE, retryonratelimit = TRUE\n",
    ")\n",
    "Sys.time()\n",
    "dim(nhsteyer)\n",
    "fwrite(nhsteyer,file=\"C:/Users/Evanm/Documents/nhsteyer.csv\")\n",
    "\n",
    "\n",
    "# Since I cleared the environment, I have to upload them back\n",
    "getwd()\n",
    "nhbernie <- read.csv(\"C:/Users/Evanm/Documents/nhbernie.csv\")\n",
    "View(nhbernie)\n",
    "\n",
    "###################### NEVADA\n",
    "nevada = lookup_coords(address=\"nevada\")\n",
    "Sys.time()\n",
    "nvbernie <- search_tweets(\n",
    "  \"#bernie OR #bernie2020 OR bernie OR sanders\", geocode = nevada, n = 100000,\n",
    "  include_rts = FALSE, retryonratelimit = TRUE\n",
    ")\n",
    "Sys.time()\n",
    "dim(nvbernie)\n",
    "fwrite(nvbernie,file=\"C:/Users/Evanm/Documents/nvbernie.csv\")\n",
    "\n",
    "Sys.time()\n",
    "nvbiden <- search_tweets(\n",
    "  \"#biden OR #biden2020 OR biden OR joe\", geocode = nevada, n = 100000,\n",
    "  include_rts = FALSE, retryonratelimit = TRUE\n",
    ")\n",
    "Sys.time()\n",
    "dim(nvbiden)\n",
    "fwrite(nvbiden,file=\"C:/Users/Evanm/Documents/nvbiden.csv\")\n",
    "\n",
    "Sys.time()\n",
    "nvklobuchar <- search_tweets(\n",
    "  \"#klobuchar OR #klobuchar2020 OR klobuchar OR amy\", geocode = nevada, n = 100000,\n",
    "  include_rts = FALSE, retryonratelimit = TRUE\n",
    ")\n",
    "Sys.time()\n",
    "dim(nvklobuchar)\n",
    "fwrite(nvklobuchar,file=\"C:/Users/Evanm/Documents/nvklobuchar.csv\")\n",
    "\n",
    "Sys.time()\n",
    "nvbuttigieg <- search_tweets(\n",
    "  \"#buttigieg OR #pete OR #buttigieg2020 OR #pete2020 OR buttigieg OR pete\", geocode = nevada, n = 100000,\n",
    "  include_rts = FALSE, retryonratelimit = TRUE\n",
    ")\n",
    "Sys.time()\n",
    "dim(nvbuttigieg)\n",
    "fwrite(nvbuttigieg,file=\"C:/Users/Evanm/Documents/nvbuttigieg.csv\")\n",
    "\n",
    "Sys.time()\n",
    "nvwarren <- search_tweets(\n",
    "  \"#warren OR or #warren2020 OR warren OR elizabeth\", geocode = nevada, n = 100000,\n",
    "  include_rts = FALSE, retryonratelimit = TRUE\n",
    ")\n",
    "Sys.time()\n",
    "dim(nvwarren)\n",
    "fwrite(nvwarren,file=\"C:/Users/Evanm/Documents/nvwarren.csv\")\n",
    "\n",
    "Sys.time()\n",
    "nvsteyer <- search_tweets(\n",
    "  \"#steyer OR #steyer2020 OR steyer OR tom\", geocode = nevada, n = 100000,\n",
    "  include_rts = FALSE, retryonratelimit = TRUE\n",
    ")\n",
    "Sys.time()\n",
    "dim(nvsteyer)\n",
    "fwrite(nvsteyer,file=\"C:/Users/Evanm/Documents/nvsteyer.csv\")\n",
    "\n",
    "states <- c(\"alabama\",\"arkansas\",\"california\",\"colorado\",\"maine\",\"massachusetts\",\"minnesota\",\"north carolina\",\"oklahoma\",\"tennessee\",\"texas\",\"utah\",\"vermont\",\"virginia\")\n",
    "\n",
    "candidates <- c(\"bernie\",\"steyer\",\"buttigieg\",\"warren\",\"klobuchar\",\"biden\")\n",
    "\n",
    "\n",
    "# Write a for loop for super Tuesday\n",
    "# Here is a loop that goes over all states from a list of states for super tuesday.\n",
    "# a new loop is done for each candidate. Alternatively I could have created a nested loop but I was running into issues with that\n",
    "\n",
    "for (st in states) {\n",
    "  state = lookup_coords(address=st)\n",
    "  ststeyer <- paste(st,\"steyer\",sep=\"_\")\n",
    "  ststeyer <- search_tweets(\n",
    "    \"#steyer OR #steyer2020 OR steyer OR tom\", geocode = state, n = 10,\n",
    "    include_rts = FALSE, retryonratelimit = TRUE\n",
    "  )\n",
    "  fwrite(ststeyer,file=paste(\"C:/Users/Evanm/Documents/\",st, \"steyer.csv\",sep='_'))\n",
    " \n",
    " \n",
    "}\n",
    "for (st in states) {\n",
    "  state = lookup_coords(address=st)\n",
    " \n",
    "  ststeyer <- paste(st,\"steyer\",sep=\"_\")\n",
    "  ststeyer <- search_tweets(\n",
    "    \"#steyer OR #steyer2020 OR steyer OR tom\", geocode = state, n = 10,\n",
    "    include_rts = FALSE, retryonratelimit = TRUE\n",
    "  )\n",
    "  fwrite(paste(st,\"steyer\",sep=\"_\"),file=paste(\"C:/Users/Evanm/Documents/\",st, \"steyer\",sep='_'))\n",
    "}\n",
    "\n",
    "# Now lets think about the sentiment per person\n",
    "# Tells the times tweets were made\n",
    "nvbernie$created_at\n",
    "# converts those times into year, month, day, hour, minute, second time variables\n",
    "nvbernie$created_at <- ymd_hms(nvbernie$created_at)\n",
    "# filters for specific times\n",
    "bernietime <- nvbernie %>%\n",
    "  filter(created_at >= as.Date(\"2020-02-20 00:00:00\") & created_at <= as.Date(\"2020-02-22 8:00:00 UTC\"))\n",
    "# again, here we can get the sentiment of tweets that include bernie's name\n",
    "berniesents <- get_sentences(bernietime$text %>% as.character())\n",
    "berniesent <- sentiment(berniesents)\n",
    "\n",
    "# First we should subset away all neutral sentiments. These are usually descriptive and will make our results less stark\n",
    "berniesub <- subset(berniesent,sentiment != 0)\n",
    "meanbernie <- mean(berniesub$sentiment)\n",
    "\n",
    "meanbernie\n",
    "\n",
    "############# BIDEN\n",
    "nvbiden$created_at <- ymd_hms(nvbiden$created_at)\n",
    "bidentime <- nvbiden %>%\n",
    "  filter(created_at >= as.Date(\"2020-02-20 00:00:00\") & created_at <= as.Date(\"2020-02-22 8:00:00 UTC\"))\n",
    "\n",
    "bidensents <- get_sentences(bidentime$text %>% as.character())\n",
    "bidensent <- sentiment(bidensents)\n",
    "\n",
    "# First we should subset away all neutral sentiments. These are usually descriptive and will make our results less stark\n",
    "bidensub <- subset(bidensent,sentiment != 0)\n",
    "meanbiden <- mean(bidensub$sentiment)\n",
    "\n",
    "meanbiden\n",
    "\n",
    "\n",
    "####### buttigieg\n",
    "nvbuttigieg$created_at <- ymd_hms(nvbuttigieg$created_at)\n",
    "buttigiegtime <- nvbuttigieg %>%\n",
    "  filter(created_at >= as.Date(\"2020-02-20 00:00:00\") & created_at <= as.Date(\"2020-02-22 8:00:00 UTC\"))\n",
    "\n",
    "buttigiegsents <- get_sentences(buttigiegtime$text %>% as.character())\n",
    "buttigiegsent <- sentiment(buttigiegsents)\n",
    "\n",
    "# First we should subset away all neutral sentiments. These are usually descriptive and will make our results less stark\n",
    "buttigiegsub <- subset(buttigiegsent,sentiment != 0)\n",
    "meanbuttigieg <- mean(buttigiegsub$sentiment)\n",
    "\n",
    "meanbuttigieg\n",
    "\n",
    "meanbernie\n",
    "\n",
    "\n",
    "\n",
    "# Next we need to subset our dates. We want only the 24 hours before the NH primary.\n",
    "\n",
    "# Now lets think about the sentiment per person again\n",
    "getwd()\n",
    "nvbernie <- fread(\"C:/Users/Evanm/Documents/nvbernie.csv\")\n",
    "results <- read.csv(\"C:/Users/Evanm/Downloads/results.csv\")\n",
    "\n",
    "nvbernie$created_at\n",
    "nvbernie$created_at <- ymd_hms(nvbernie$created_at)\n",
    "bernietime <- nvbernie %>%\n",
    "  filter(created_at >= as.Date(\"2020-02-20 00:00:00\") & created_at <= as.Date(\"2020-02-22 8:00:00 UTC\"))\n",
    "max(bernietime$created_at)\n",
    "dim(bernietime)\n",
    "berniesents <- get_sentences(bernietime$text %>% as.character())\n",
    "berniesent <- sentiment(berniesents)\n",
    "\n",
    "# First we should subset away all neutral sentiments. These are usually descriptive and will make our results less stark\n",
    "berniesub <- subset(berniesent,sentiment != 0)\n",
    "meanbernie <- mean(berniesub$sentiment)\n",
    "\n",
    "meanbernie\n",
    "\n",
    "finsent\n",
    "ntweets\n",
    "\n",
    "# NEVADA\n",
    "nvntweets <- matrix()\n",
    "colnames(nvntweets) <- c(\"candidate\",\"ntweets\")\n",
    "nvfinsent <- matrix()\n",
    "for (can in candidates) {\n",
    "  int <- paste(\"nv\",can,sep=\"\")\n",
    "  csvname <- paste(int,\".csv\",sep=\"\")\n",
    "  dat <- read.csv(paste(\"C:/Users/Evanm/Documents/\", csvname,sep = \"\"))\n",
    "  dat$created_at <- ymd_hms(dat$created_at)\n",
    "  cantime <- dat %>%\n",
    "    filter(created_at >= as.Date(\"2020-02-20 00:00:00\") & created_at <= as.Date(\"2020-02-22 00:00:00 UTC\"))\n",
    "  nvntweets[can] <- nrow(cantime)\n",
    "  cansents <- get_sentences(cantime$text %>% as.character())\n",
    "  cansent <- sentiment(cansents)\n",
    "  cansub <- subset(cansent,sentiment != 0)\n",
    "  meancan <- mean(cansub$sentiment)\n",
    "  nvfinsent[can] <- meancan\n",
    "}\n",
    "nvntweets\n",
    "nvfinsent\n",
    "# returns sentiment and number of tweets, we can combine those out here as we please.\n",
    "\n",
    "# NEW HAMPSHIRE has very little data due to poor timing of projet. Have to throw out.\n",
    "\n",
    "tweets <- data.frame(nvntweets)\n",
    "\n",
    "sent <- data.frame(nvfinsent)\n",
    "\n",
    "tweets[-c(1),]\n",
    "View(tweets)\n",
    "dat <- cbind(tweets,sent) %>% as.matrix\n",
    "dat <- dat[-1,]\n",
    "# mat <- tweets %>% row.names %>% as.matrix\n",
    "# mat <- mat[-1]\n",
    "# mat %<>% tibble\n",
    "# dat <- cbind(mat,dat)\n",
    "#\n",
    "# dat %>% tibble %>% rename(c(\"candidates\" =\".\"))\n",
    "#\n",
    "\n",
    "results\n",
    "nevresults <- subset(results, state == \"nevada\")\n",
    "\n",
    "dat <- cbind(dat,nevresults)\n",
    "\n",
    "dat %>% as.data.frame()\n",
    "\n",
    "mydat <- dat %>% as.data.frame()\n",
    "library(estimatr)\n",
    "lm_robust(result ~ nvntweets + nvfinsent + nvntweets*nvfinsent,data = dat)\n",
    "\n",
    "colnames(mydat)\n",
    "\n",
    "colnames(results)\n",
    "\n",
    "?rename\n",
    "nrow(nvbernie)\n",
    "length\n",
    "for (st in states) {\n",
    "  state = lookup_coords(address=st)\n",
    " \n",
    "  ststeyer <- paste(st,\"steyer\",sep=\"_\")\n",
    "  ststeyer <- search_tweets(\n",
    "    \"#steyer OR #steyer2020 OR steyer OR tom\", geocode = state, n = 10,\n",
    "    include_rts = FALSE, retryonratelimit = TRUE\n",
    "  )\n",
    "  fwrite(paste(st,\"steyer\",sep=\"_\"),file=paste(\"C:/Users/Evanm/Documents/\",st, \"steyer\",sep='_'))\n",
    "}\n",
    "\n",
    "\n",
    "################################# Time separation\n",
    "\n",
    "round_time <- function(x, secs) as.POSIXct(hms::round_hms(x, secs))\n",
    "## function to calculate sentiment scores\n",
    "sent_scores <- function(x) syuzhet::get_sentiment(plain_tweets(x)) - .5\n",
    "## calc data set with sentiment variable\n",
    "warren <- fread(\"C:/Users/Evan Jonson/Downloads/_alabama_warren.csv\")\n",
    "\n",
    "######################### THIS WORKS\n",
    "# this creates new columns called 'hours' which is the rounded hour the tweet was made and 'sentiment' which is the sentiment that tweet recieved\n",
    "tt_sent <- warren %>%\n",
    "  mutate(hours = round_time(created_at %>% ymd_hms(), 60 * 60 * 24),\n",
    "         sentiment = sent_scores(text))\n",
    "\n",
    "# THIS WORKS TOO\n",
    "# this aggregates into individual hours and takes the mean of each hour\n",
    "\n",
    "aggregate(canstate$sentiment,by=list(canstate$hours),FUN=mean)\n",
    "\n",
    "# Now lets make a loop out of it, for every state for each individual candidate.\n",
    "# warren\n",
    "for (state in states) {\n",
    "  int <- paste(\"_\",state,\"_\",\"warren\",sep=\"\")\n",
    "  csvname <- paste(int,\".csv\",sep=\"\")\n",
    "  dat <- fread(paste(\"C:/Users/Evan Jonson/Downloads/\", csvname,sep = \"\"))\n",
    "  frame <- as.data.frame(dat)\n",
    "  canstate <- paste(state, \"warrentime\",sep = \"_\")\n",
    "  canstate <- frame %>%\n",
    "    mutate(hours = round_time(created_at %>% ymd_hms(), 60 * 60 * 24),\n",
    "           sentiment = sent_scores(text))\n",
    "  agg <- aggregate(canstate$sentiment,by=list(canstate$hours),FUN=mean) # aggregates sentiment by hour\n",
    "  numbagg <- aggregate(canstate$sentiment,by=list(canstate$hours),FUN=length)\n",
    "  finalagg <- cbind(agg,numbagg$x)\n",
    "#  finalagg <- aggregate(canstate$sentiment,by=list(canstate$hours),FUN=mean)\n",
    "  fwrite(finalagg,file = paste(state, \"warrentime.csv\",sep = \"_\"))\n",
    "  print(state)\n",
    "}\n",
    "\n",
    "# now Bernie\n",
    "for (state in states) {\n",
    "  int <- paste(\"_\",state,\"_\",\"bernie\",sep=\"\")\n",
    "  csvname <- paste(int,\".csv\",sep=\"\")\n",
    "  dat <- fread(paste(\"C:/Users/Evan Jonson/Downloads/\", csvname,sep = \"\"))\n",
    "  frame <- as.data.frame(dat)\n",
    "  canstate <- paste(state, \"bernietime\",sep = \"_\")\n",
    "  canstate <- frame %>%\n",
    "    mutate(hours = round_time(created_at %>% ymd_hms(), 60 * 60 * 24),\n",
    "           sentiment = sent_scores(text))\n",
    "  agg <- aggregate(canstate$sentiment,by=list(canstate$hours),FUN=mean)\n",
    "  numbagg <- aggregate(canstate$sentiment,by=list(canstate$hours),FUN=length)\n",
    "  finalagg <- cbind(agg,numbagg$x)  \n",
    "  fwrite(finalagg,file = paste(state, \"bernietime.csv\",sep = \"_\"))\n",
    "  print(state)\n",
    "}\n",
    "\n",
    "# Biden\n",
    "for (state in states) {\n",
    "  int <- paste(\"_\",state,\"_\",\"biden\",sep=\"\")\n",
    "  csvname <- paste(int,\".csv\",sep=\"\")\n",
    "  dat <- fread(paste(\"C:/Users/Evan Jonson/Downloads/\", csvname,sep = \"\"))\n",
    "  frame <- as.data.frame(dat)\n",
    "  canstate <- paste(state, \"bidentime\",sep = \"_\")\n",
    "  canstate <- frame %>%\n",
    "    mutate(hours = round_time(created_at %>% ymd_hms(), 60 * 60 * 24),\n",
    "           sentiment = sent_scores(text))\n",
    "  agg <- aggregate(canstate$sentiment,by=list(canstate$hours),FUN=mean)\n",
    "  numbagg <- aggregate(canstate$sentiment,by=list(canstate$hours),FUN=length)\n",
    "  finalagg <- cbind(agg,numbagg$x)  \n",
    "  fwrite(finalagg,file = paste(state, \"bidentime.csv\",sep = \"_\"))\n",
    "  print(state)\n",
    "}\n",
    "\n",
    "# Bloomberg\n",
    "for (state in states) {\n",
    "  int <- paste(\"_\",state,\"_\",\"bloomberg\",sep=\"\")\n",
    "  csvname <- paste(int,\".csv\",sep=\"\") # Intermediate step, wouldnt let me work otherwise\n",
    "  dat <- fread(paste(\"C:/Users/Evan Jonson/Downloads/\", csvname,sep = \"\")) # read the CSV's in\n",
    "  frame <- as.data.frame(dat) # data frame for easier processing\n",
    "  canstate <- paste(state, \"bloombergtime\",sep = \"_\")\n",
    "  canstate <- frame %>%\n",
    "    mutate(hours = round_time(created_at %>% ymd_hms(), 60 * 60 * 24), # Creates hourly times\n",
    "           sentiment = sent_scores(text)) # creates sentiment score\n",
    "  agg <- aggregate(canstate$sentiment,by=list(canstate$hours),FUN=mean) # aggregates sentiment by hour\n",
    "  numbagg <- aggregate(canstate$sentiment,by=list(canstate$hours),FUN=length)\n",
    "  finalagg <- cbind(agg,numbagg$x)\n",
    "  fwrite(finalagg,file = paste(state, \"bloombergtime.csv\",sep = \"_\")) # write to csv for later\n",
    "  print(state) # as a sort of progress bar\n",
    "}\n",
    "\n",
    "\n",
    "# So we have sentiment by hour. Another of these was done to get the number of tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is our models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "'/kaggle/working'"
      ],
      "text/latex": [
       "'/kaggle/working'"
      ],
      "text/markdown": [
       "'/kaggle/working'"
      ],
      "text/plain": [
       "[1] \"/kaggle/working\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ERROR",
     "evalue": "Error in setwd(\"C:/Users/Evanm/Downloads\"): cannot change working directory\n",
     "output_type": "error",
     "traceback": [
      "Error in setwd(\"C:/Users/Evanm/Downloads\"): cannot change working directory\nTraceback:\n",
      "1. setwd(\"C:/Users/Evanm/Downloads\")"
     ]
    }
   ],
   "source": [
    "# Final Final Preds\n",
    "library(pacman)\n",
    "p_load(Metrics,kernlab,BiocManager,glmnet,caret,ranger,randomForest,tidyverse, rtweet,sentimentr,data.table,lubridate,janitor,magrittr,stringr,estimatr)\n",
    "# note, use kernlab for SVM in general.\n",
    "\n",
    "lambdas = 10^seq(from = 3, to = -5, length = 1000)\n",
    "# Fit ridge regression\n",
    "getwd()\n",
    "setwd(\"C:/Users/Evanm/Downloads\")\n",
    "final_results <- read.csv(\"final_data.csv\")\n",
    "\n",
    "# Try with and without standardization\n",
    "\n",
    "mylm <- lm_robust(results ~.,data=final_results %>% select(-X))\n",
    "\n",
    "colnames(final_results)\n",
    "\n",
    "summary(lm)\n",
    "\n",
    "# Divide up data into test and train\n",
    "require(caTools)\n",
    "set.seed(101) \n",
    "# Should we split up data like this? Maybe split and predict a specific state.\n",
    "sample = sample.split(final_results$X, SplitRatio = .75)\n",
    "train = subset(final_results, sample == TRUE)\n",
    "test  = subset(final_results, sample == FALSE)\n",
    "\n",
    "# Try MAE and MSE\n",
    "# How, exactly, can we tell how accurate this is.\n",
    "lasso_cv = cv.glmnet(\n",
    "  x = final_results %>% dplyr::select(-results,-X) %>% as.matrix(),\n",
    "  y = final_results$results,\n",
    "  standardize = T,\n",
    "  alpha = 1,\n",
    "  lambda = lambdas,\n",
    "  # New: How we make decisions and number of folds\n",
    "  type.measure = \"mse\",\n",
    "  nfolds = 5\n",
    ")\n",
    "dim(final_results)\n",
    "est_lasso = glmnet(\n",
    "  x = final_results %>% dplyr::select(-results,-X) %>% as.matrix(),\n",
    "  y = final_results$results,\n",
    "  standardize = T,\n",
    "  alpha = 1,\n",
    "  lambda = lasso_cv$lambda.min\n",
    ")\n",
    "varImp(est_lasso)\n",
    "coef(est_lasso)\n",
    "\n",
    "\n",
    "preds <- predict(\n",
    "  lasso_cv,\n",
    "  type = \"response\",\n",
    "  # Our chosen lambda\n",
    "  s = lasso_cv$lambda.min,\n",
    "  # Our data\n",
    "  newx = final_results %>% dplyr::select(-results,-X) %>% as.matrix()\n",
    ")\n",
    "RMSE(pred = preds, obs = final_results$results)\n",
    "MAE(pred = preds, obs = final_results$results)\n",
    "\n",
    "\n",
    "preds <- cbind(preds,test$X)\n",
    "varImp(lasso_cv)\n",
    "## Boosted tree\n",
    "\n",
    "# Set the seed\n",
    "set.seed(12345)\n",
    "# Train the random forest\n",
    "boosted = train(\n",
    "  results ~ .,\n",
    "  data = final_results %>% select(-X),\n",
    "  method = \"gbm\",\n",
    "  trControl = trainControl(\n",
    "    method = \"cv\",\n",
    "    number = 5\n",
    "  ),\n",
    "  tuneGrid = expand.grid(\n",
    "    \"n.trees\" = seq(25, 1000, by = 25),\n",
    "    \"interaction.depth\" = 1:5,\n",
    "    \"shrinkage\" = c(0.1, 0.01, 0.001),\n",
    "    \"n.minobsinnode\" = 5\n",
    "  )\n",
    ")\n",
    "boosted$bestTune\n",
    "boosted$finalModel\n",
    "\n",
    "aboost=gbm(results ~ .,data = final_results %>% select(-X),distribution = \"gaussian\",n.trees = 350,\n",
    "              shrinkage = 0.01, interaction.depth = 4)\n",
    "View(aboost)\n",
    "# How to get preds and error rate out of this, how to interpret\n",
    "aboost$fit\n",
    "p_load(gbm)\n",
    "myboosted=gbm(results ~ .,data = final_results %>% select(-X),distribution = \"gaussian\",n.trees = 500,\n",
    "                 shrinkage = 0.01, interaction.depth = 2)\n",
    "boostpreds <- predict(\n",
    "  myboosted,\n",
    "  type = \"response\",\n",
    "  n.trees = 500,\n",
    "  # Our data\n",
    "  newx = final_results %>% dplyr::select(-results,-X) %>% as.matrix()\n",
    ")\n",
    "RMSE(pred = boostpreds, obs = final_results$results)\n",
    "MAE(pred = boostpreds, obs = final_results$results)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "myboosted\n",
    "summary(myboosted)\n",
    "plot(myboosted$oobag.improve)\n",
    "plot(myboosted$m)\n",
    "\n",
    "# try to quantify\n",
    "# talk about rmse instead\n",
    "# show our predictions\n",
    "\n",
    "\n",
    "boosted$finalModel\n",
    "View(boosted)\n",
    "varImp(boosted)\n",
    "\n",
    "## Random forest\n",
    "\n",
    "\n",
    "# Train the random forest\n",
    "forest = train(\n",
    "  results ~ .,\n",
    "  data = final_results %>% select(-X),\n",
    "  method = \"ranger\",\n",
    "  num.trees = 500,\n",
    "  trControl = trainControl(\n",
    "    method = \"oob\"\n",
    "  ),\n",
    "  tuneGrid = expand.grid(\n",
    "    \"mtry\" = 2:13,\n",
    "    \"splitrule\" = \"variance\",\n",
    "    \"min.node.size\" = 1:10\n",
    "  )\n",
    ")\n",
    "varImp(forest)\n",
    "coef(forest)\n",
    "# Create the actual forest\n",
    "myforest <- ranger(results ~ ., data = final_results %>% select(-X),num.trees = 1000,mtry = 11,min.node.size = 3)\n",
    "\n",
    "# oob of 106.29\n",
    "\n",
    "myforest <- randomForest(results ~ ., data = final_results %>% select(-X),ntree = 1000,mtry = 11,min.node.size = 3,importance = TRUE)\n",
    "\n",
    "forestpreds <- predict(\n",
    "  myforest,\n",
    "  type = \"response\",\n",
    "  n.trees = 1000,\n",
    "  # Our data\n",
    "  newx = final_results %>% dplyr::select(-results,-X) %>% as.matrix()\n",
    ")\n",
    "RMSE(pred = forestpreds, obs = final_results$results)\n",
    "MAE(pred = forestpreds, obs = final_results$results)\n",
    "\n",
    "forestimps <- importance(myforest)\n",
    "forestimps %<>% as.data.frame\n",
    "senttt <- forestimps %>% dplyr::select(-\"%IncMSE\")\n",
    "\n",
    "sorted <- forestimps[-order(\"IncNodePurity\"),]\n",
    "\n",
    "plot(forestimps$IncNodePurity)\n",
    "\n",
    "svmfit <- svm(results~., data = final_results %>% select(-X), kernel = \"radial\", scale = FALSE,cost=10)\n",
    "\n",
    "library(kernlab)\n",
    "# Set a seed\n",
    "set.seed(12345)\n",
    "# Tune radial\n",
    "newpolysvm = train(\n",
    "  results ~ .,\n",
    "  data = final_results %>% select(-X),\n",
    "  method = \"svmPoly\",\n",
    "  scaled = T,\n",
    "  metric = \"RMSE\",\n",
    "  trControl = trainControl(\n",
    "    method = \"cv\",\n",
    "    number = 5\n",
    "  ),\n",
    "  tuneGrid = expand.grid(\n",
    "  #  sigma = c(0.1, 1, 5, 10, 20),\n",
    "    C = 10^seq(-7, -4, by = 1),\n",
    "    degree = 1,\n",
    "    scale = c(.5)\n",
    "  )\n",
    ")\n",
    "\n",
    "min(polysvm$results$RMSE)\n",
    "\n",
    "\n",
    "mysvm <- kernlab::ksvm(\n",
    "  results ~ .,\n",
    "  data = final_results %>% select(-X),\n",
    "  C = .1,\n",
    "  degree = 1,\n",
    "  scale = .5\n",
    ")\n",
    "\n",
    "svmpoly <- train(\n",
    "  results ~ .,\n",
    "  data = final_results %>% select(-X),\n",
    "  method = \"svmPoly\",  trControl = trainControl(\"cv\", number = 10),\n",
    "  preProcess = c(\"center\",\"scale\"),\n",
    "  tuneLength = 4\n",
    ")\n",
    "\n",
    "svmpreds <- predict(\n",
    "  svmpoly,\n",
    "  type = \"raw\",\n",
    "  # Our data\n",
    "  newx = final_results %>% dplyr::select(-results,-X) %>% as.matrix()\n",
    ")\n",
    "RMSE(pred = svmpreds, obs = final_results$results)\n",
    "MAE(pred = svmpreds, obs = final_results$results)\n",
    "\n",
    "\n",
    "\n",
    "# SVM\n",
    "\n",
    "# Predict\n",
    "predict(mysvm, newdata = final_results)\n",
    "# TERRIBLE PREDICTIONS\n",
    "\n",
    "trctrl <- trainControl(method = \"repeatedcv\", number = 5, repeats = 3)\n",
    "knn_fit <- train(results ~ .,\n",
    "                 data = final_results %>% select(-X),\n",
    "                 method = \"knn\",\n",
    "                 trControl=trctrl,\n",
    "                 preProcess = c(\"center\", \"scale\"),\n",
    "                 tuneLength = 10)\n",
    "knn_fit\n",
    "\n",
    "knnpreds <- predict(\n",
    "  knn_fit,\n",
    "  type = \"raw\",\n",
    "  # Our data\n",
    "  newx = final_results %>% dplyr::select(-results,-X) %>% as.matrix()\n",
    ")\n",
    "RMSE(pred = knnpreds, obs = final_results$results)\n",
    "MAE(pred = knnpreds, obs = final_results$results)\n",
    "\n",
    "\n",
    "\n",
    "masdf <- varImp(knn_fit)\n",
    "class(masdf)\n",
    "\n",
    "plot(varImp(knn_fit))\n",
    "\n",
    "### Decision tree\n",
    "# Set seed\n",
    "set.seed(12345)\n",
    "# CV and train\n",
    "default_tree = train(\n",
    "  results ~ .,\n",
    "  data = final_results %>% select(-X),\n",
    "  method = \"rpart\",\n",
    "  trControl = trainControl(\"cv\", number = 5),\n",
    "  tuneGrid = data.frame(cp = seq(0, 0.2, by = 0.005))\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
